# ABLNet
ABLNet: Adaptive Bias Learning Network for Visual Question Answering

Visual question answering (VQA) for understanding images and matching questions has become a promising research direction in the cross-modal analysis community. Existing works highly rely on the knowledge of the datasets, some factors, i.e., spurious correlations, dataset biases, and learning shortcuts have a significant impact on algorithm robustness. In this work, enhance the bias model's ability to learn biases in the dataset, we propose a novel framework named an adaptive bias learning network (ABLNet) for VQA.  We introduce the gradient during bias model training to quantify the level of bias in each sample. Concretely, we assign higher weights to biased samples and lower weights to unbiased samples to enhance the learning of dataset bias.  Additionally, we designed a weight pruning strategy to deliberately weaken the learning capacity of the bias model compared to the target model, facilitating improved bias learning.  Extensive experiments conducted on three challenging VQA datasets, i.e., VQA-CPv2, VQA-v2 and GQA-OOD, indicate superior performance compared with most state-of-the-art methods. The code will be released after the paper is published.
